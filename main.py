"""
AI 101 - ì§€ëŠ¥í˜• AI ë„êµ¬ ì¶”ì²œ ì—ì´ì „íŠ¸
FastAPI + Gradio UI í†µí•© ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
"""
import os
import json
import uuid
import asyncio
from typing import Optional, Generator, AsyncGenerator
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import gradio as gr
from dotenv import load_dotenv

from src.graph import (
    create_agent_graph,
    create_initial_state,
    get_memory_manager
)
from src.prompts import format_plan_summary

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


# ==================== FastAPI ì„¤ì • ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” ë¡œì§"""
    # ì‹œì‘ ì‹œ: AI ë„êµ¬ ë°ì´í„° ë¡œë“œ
    print("AI 101 ì—ì´ì „íŠ¸ ì‹œì‘...")
    memory = get_memory_manager()

    # 1. JSON ë„êµ¬ ë°ì´í„° ë¡œë“œ
    json_path = "./data/ai_tools_2025.json"
    if os.path.exists(json_path):
        count = memory.load_tools_from_json(json_path)
        print(f"AI ë„êµ¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {count}ê°œ")
    else:
        print(f"ê²½ê³ : {json_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 2. PDF ì§€ì‹ë² ì´ìŠ¤ ë¡œë“œ
    pdf_dir = "./data"
    pdf_count = memory.load_pdfs_from_directory(pdf_dir)
    print(f"PDF ì§€ì‹ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ: {pdf_count}ê°œ ì²­í¬")

    yield

    # ì¢…ë£Œ ì‹œ
    print("AI 101 ì—ì´ì „íŠ¸ ì¢…ë£Œ...")


app = FastAPI(
    title="AI 101",
    description="LangGraph ê¸°ë°˜ ì§€ëŠ¥í˜• AI ë„êµ¬ ì¶”ì²œ ì—ì´ì „íŠ¸",
    version="1.0.0",
    lifespan=lifespan
)


# ==================== API ëª¨ë¸ ====================

class ChatRequest(BaseModel):
    query: str
    user_id: str = "default_user"
    thread_id: Optional[str] = None


class ApproveRequest(BaseModel):
    thread_id: str
    action: str = "approve"  # approve, modify, cancel
    feedback: Optional[str] = None


class ChatResponse(BaseModel):
    thread_id: str
    status: str
    message: str
    plan: Optional[list] = None
    final_response: Optional[str] = None


# ==================== ì„¸ì…˜ ê´€ë¦¬ ====================

# í™œì„± ì„¸ì…˜ ì €ì¥ (thread_id -> graph_state)
active_sessions = {}


# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    memory = get_memory_manager()
    return {
        "status": "healthy",
        "tools_count": memory.get_tools_count(),
        "profiles_count": memory.get_profiles_count()
    }


@app.post("/chat/start", response_model=ChatResponse)
async def start_chat(request: ChatRequest):
    """
    ì±„íŒ… ì‹œì‘ - Plan ë‹¨ê³„ê¹Œì§€ ì‹¤í–‰ í›„ ìŠ¹ì¸ ëŒ€ê¸°
    """
    thread_id = request.thread_id or str(uuid.uuid4())

    try:
        graph = create_agent_graph()
        config = {"configurable": {"thread_id": thread_id}}
        initial_state = create_initial_state(request.query, request.user_id)

        # Plan ë‹¨ê³„ê¹Œì§€ ì‹¤í–‰ (human_review ì „ì— interrupt)
        final_state = None
        for event in graph.stream(initial_state, config):
            for node_name, node_output in event.items():
                print(f"[{thread_id}] Node: {node_name}")
            final_state = event

        # í˜„ì¬ ìƒíƒœ ì¡°íšŒ
        state = graph.get_state(config)
        subtasks = state.values.get("subtasks", [])
        plan_analysis = state.values.get("plan_analysis", "")

        # ì„¸ì…˜ ì €ì¥
        active_sessions[thread_id] = {
            "graph": graph,
            "config": config,
            "state": state
        }

        return ChatResponse(
            thread_id=thread_id,
            status="pending_approval",
            message=f"ì‘ì—… ê³„íšì„ ìˆ˜ë¦½í–ˆìŠµë‹ˆë‹¤.\n\në¶„ì„: {plan_analysis}\n\nìŠ¹ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
            plan=[{
                "id": t.get("id"),
                "description": t.get("description"),
                "category": t.get("category")
            } for t in subtasks]
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/approve", response_model=ChatResponse)
async def approve_plan(request: ApproveRequest):
    """
    ê³„íš ìŠ¹ì¸ í›„ ë‚˜ë¨¸ì§€ ë‹¨ê³„ ì‹¤í–‰
    """
    thread_id = request.thread_id

    if thread_id not in active_sessions:
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    session = active_sessions[thread_id]
    graph = session["graph"]
    config = session["config"]

    try:
        # ì‚¬ìš©ì ì‘ë‹µìœ¼ë¡œ ê·¸ë˜í”„ ì¬ê°œ
        user_response = {
            "action": request.action,
            "feedback": request.feedback or ""
        }

        # Commandë¡œ interrupt ì¬ê°œ
        from langgraph.types import Command

        final_state = None
        for event in graph.stream(Command(resume=user_response), config):
            for node_name, node_output in event.items():
                print(f"[{thread_id}] Node: {node_name}")
            final_state = event

        # ìµœì¢… ìƒíƒœ ì¡°íšŒ
        state = graph.get_state(config)
        final_response = state.values.get("final_response", "")

        # ì„¸ì…˜ ì •ë¦¬
        if thread_id in active_sessions:
            del active_sessions[thread_id]

        if request.action == "cancel":
            return ChatResponse(
                thread_id=thread_id,
                status="cancelled",
                message="ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                final_response=None
            )

        return ChatResponse(
            thread_id=thread_id,
            status="completed",
            message="ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            final_response=final_response
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ==================== Gradio UI ====================

def create_gradio_ui():
    """Gradio ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    def chat_start(message: str, history: list, user_id: str):
        """ì±„íŒ… ì‹œì‘ - Plan ìƒì„±"""
        if not message.strip():
            return history, None, None, "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

        thread_id = str(uuid.uuid4())

        try:
            graph = create_agent_graph()
            config = {"configurable": {"thread_id": thread_id}}
            initial_state = create_initial_state(message, user_id or "gradio_user")

            # Plan ë‹¨ê³„ê¹Œì§€ ì‹¤í–‰
            for event in graph.stream(initial_state, config):
                pass

            state = graph.get_state(config)
            subtasks = state.values.get("subtasks", [])
            plan_analysis = state.values.get("plan_analysis", "")

            # ì„¸ì…˜ ì €ì¥
            active_sessions[thread_id] = {
                "graph": graph,
                "config": config,
                "state": state
            }

            # ê³„íš ë©”ì‹œì§€ ìƒì„±
            plan_text = f"**ğŸ“‹ ì‘ì—… ë¶„ì„**\n{plan_analysis}\n\n**ğŸ“ ìˆ˜ë¦½ëœ ê³„íš:**\n"
            for task in subtasks:
                plan_text += f"- **{task['id']}**: {task['description']} ({task['category']})\n"
            plan_text += "\n*ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ìŠ¹ì¸/ì·¨ì†Œí•´ì£¼ì„¸ìš”.*"

            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            history = history or []
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": plan_text})

            return history, thread_id, subtasks, "ê³„íš ìŠ¹ì¸ ëŒ€ê¸° ì¤‘..."

        except Exception as e:
            history = history or []
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"})
            return history, None, None, f"ì˜¤ë¥˜: {str(e)}"

    def approve_plan(history: list, thread_id: str, feedback: str):
        """ê³„íš ìŠ¹ì¸"""
        if not thread_id or thread_id not in active_sessions:
            return history, None, None, "í™œì„± ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤."

        session = active_sessions[thread_id]
        graph = session["graph"]
        config = session["config"]

        try:
            from langgraph.types import Command

            user_response = {"action": "approve", "feedback": feedback or ""}

            # ê·¸ë˜í”„ ì¬ê°œ
            for event in graph.stream(Command(resume=user_response), config):
                pass

            state = graph.get_state(config)
            final_response = state.values.get("final_response", "ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            history.append({"role": "assistant", "content": f"âœ… **ê³„íš ìŠ¹ì¸ë¨**\n\n{final_response}"})

            # ì„¸ì…˜ ì •ë¦¬
            if thread_id in active_sessions:
                del active_sessions[thread_id]

            return history, None, None, "ì™„ë£Œ!"

        except Exception as e:
            history.append({"role": "assistant", "content": f"ì˜¤ë¥˜: {str(e)}"})
            return history, None, None, f"ì˜¤ë¥˜: {str(e)}"

    def cancel_plan(history: list, thread_id: str):
        """ê³„íš ì·¨ì†Œ"""
        if thread_id and thread_id in active_sessions:
            del active_sessions[thread_id]

        history.append({"role": "assistant", "content": "âŒ **ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.**"})
        return history, None, None, "ì·¨ì†Œë¨"

    # UI êµ¬ì„±
    with gr.Blocks(
        title="AI 101 - AI ë„êµ¬ ì¶”ì²œ ì—ì´ì „íŠ¸"
    ) as demo:
        
        # ìƒíƒœ ë³€ìˆ˜
        current_thread_id = gr.State(None)
        current_plan = gr.State(None)

        gr.Markdown("""
        # ğŸ¤– AI 101 - ì§€ëŠ¥í˜• AI ë„êµ¬ ì¶”ì²œ ì—ì´ì „íŠ¸

        AI ë„êµ¬ë¥¼ í™œìš©í•œ ì‘ì—…ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤. ì›í•˜ëŠ” ì‘ì—…ì„ ìì—°ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”!

        **ì˜ˆì‹œ ì§ˆë¬¸:**
        - "ìœ íŠœë¸Œ ì‡¼ì¸  ë¯¸ìŠ¤í…Œë¦¬ ì˜ìƒì„ ë§Œë“¤ê³  ì‹¶ì–´"
        - "ë¸”ë¡œê·¸ ê¸€ì„ ìë™ìœ¼ë¡œ ì‘ì„±í•˜ê³  ì‹¶ì–´"
        - "AIë¡œ ë¡œê³  ë””ìì¸ì„ í•˜ê³  ì‹¶ì–´"
        """)

        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    label="ëŒ€í™”",
                    height=500
                )

                with gr.Row():
                    msg = gr.Textbox(
                        label="ë©”ì‹œì§€ ì…ë ¥",
                        placeholder="AI ë„êµ¬ ì¶”ì²œì„ ìš”ì²­í•˜ì„¸ìš”...",
                        scale=4,
                        show_label=False
                    )
                    submit_btn = gr.Button("ì „ì†¡", variant="primary", scale=1)

                with gr.Row():
                    approve_btn = gr.Button("âœ… ê³„íš ìŠ¹ì¸", variant="primary")
                    cancel_btn = gr.Button("âŒ ì·¨ì†Œ", variant="stop")

                feedback_input = gr.Textbox(
                    label="ìˆ˜ì • ìš”ì²­ (ì„ íƒì‚¬í•­)",
                    placeholder="ê³„íšì— ëŒ€í•œ ìˆ˜ì • ìš”ì²­ì´ ìˆìœ¼ë©´ ì…ë ¥í•˜ì„¸ìš”...",
                    visible=True
                )

            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ ì„¤ì •")
                user_id_input = gr.Textbox(
                    label="ì‚¬ìš©ì ID",
                    value="default_user",
                    placeholder="ì‚¬ìš©ì ID"
                )
                status_text = gr.Textbox(
                    label="ìƒíƒœ",
                    value="ëŒ€ê¸° ì¤‘",
                    interactive=False
                )

                gr.Markdown("### ğŸ“Š í†µê³„")
                with gr.Row():
                    tools_count = gr.Number(label="ë“±ë¡ëœ ë„êµ¬", value=0, interactive=False)
                    profiles_count = gr.Number(label="ì‚¬ìš©ì í”„ë¡œí•„", value=0, interactive=False)

                refresh_btn = gr.Button("ğŸ”„ ìƒˆë¡œê³ ì¹¨")

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        submit_btn.click(
            fn=chat_start,
            inputs=[msg, chatbot, user_id_input],
            outputs=[chatbot, current_thread_id, current_plan, status_text]
        ).then(
            fn=lambda: "",
            outputs=msg
        )

        msg.submit(
            fn=chat_start,
            inputs=[msg, chatbot, user_id_input],
            outputs=[chatbot, current_thread_id, current_plan, status_text]
        ).then(
            fn=lambda: "",
            outputs=msg
        )

        approve_btn.click(
            fn=approve_plan,
            inputs=[chatbot, current_thread_id, feedback_input],
            outputs=[chatbot, current_thread_id, current_plan, status_text]
        ).then(
            fn=lambda: "",
            outputs=feedback_input
        )

        cancel_btn.click(
            fn=cancel_plan,
            inputs=[chatbot, current_thread_id],
            outputs=[chatbot, current_thread_id, current_plan, status_text]
        )

        def refresh_stats():
            memory = get_memory_manager()
            return memory.get_tools_count(), memory.get_profiles_count()

        refresh_btn.click(
            fn=refresh_stats,
            outputs=[tools_count, profiles_count]
        )

        # ì´ˆê¸° ë¡œë“œ ì‹œ í†µê³„ ì—…ë°ì´íŠ¸
        demo.load(
            fn=refresh_stats,
            outputs=[tools_count, profiles_count]
        )

    return demo


# ==================== Gradio Mount ====================

# Gradio ì•± ìƒì„±
gradio_app = create_gradio_ui()

# FastAPIì— Gradio ë§ˆìš´íŠ¸
app = gr.mount_gradio_app(app, gradio_app, path="/")


# ==================== ë©”ì¸ ì‹¤í–‰ ====================

if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", 7860))
    host = os.getenv("HOST", "0.0.0.0")

    print(f"\n{'='*50}")
    print(f"AI 101 ì—ì´ì „íŠ¸ ì„œë²„ ì‹œì‘")
    print(f"URL: http://localhost:{port}")
    print(f"{'='*50}\n")

    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=True
    )
